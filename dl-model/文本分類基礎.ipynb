{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit",
      "language": "python",
      "name": "python37364bit6893c7013b164b1189a865dcaea9fb2f"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "文本分類基礎.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOmDnyP9lPO_"
      },
      "source": [
        "## 透過keras框架end-to-end建構文本分類模型\n",
        "- 資料處理\n",
        "    - 透過keras embedding layer讓模型學習text data的語意隨著訓練\n",
        "- 模型\n",
        "    - CNN-based\n",
        "    - RNN-based\n",
        "    - Transformer-based\n",
        "- 資料集\n",
        "    - 利用IMDB資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-16T23:42:25.502966Z",
          "start_time": "2021-05-16T23:42:17.043040Z"
        },
        "id": "hYoYR8nSlPPJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXk6z3sDlPPK"
      },
      "source": [
        "### 取得資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-05-16T23:42:29.919Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDA--xcxlPPL",
        "outputId": "85549d63-ec5f-4dee-cfa2-27d0f4042289"
      },
      "source": [
        "# 透過linux指令發request\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  13.3M      0  0:00:06  0:00:06 --:--:-- 18.9M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pQQ7JxSlPPL",
        "outputId": "7ec3d5e1-ac67-436d-9fbf-a372a6639154"
      },
      "source": [
        "!ls aclImdb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCXKOacfmjE4",
        "outputId": "7d402b85-4dd6-4c04-cc03-70ca7642efe0"
      },
      "source": [
        "!ls aclImdb/test"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX2UPW-pmlRs",
        "outputId": "51b9307f-1af0-4d82-bf35-a3ff1d24e893"
      },
      "source": [
        "!ls aclImdb/train"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck1Fgtthmmex",
        "outputId": "193d85cc-fe0f-4a4c-aa21-0430c3d5d9a5"
      },
      "source": [
        "# cat 指令是將每個檔案依照順序讀取並把內容送到標準輸出（螢幕）。 例如，鍵入cat filename 可將檔案filename 的內容在螢幕上顯示。\n",
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAhDsfwLmtA2"
      },
      "source": [
        "# rm 指令 刪除檔案\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6syCeskTnlUk"
      },
      "source": [
        "### 透過tf.keras.preprocessing.text_dataset_from_directory從dir取得資料建立資料集\n",
        "- Generates a tf.data.Dataset from text files in a directory.\n",
        "    - 專門用於txt資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDiAahQinT_d",
        "outputId": "9a99f4ba-c3dd-414a-ea16-bf0bd06aa75a"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    directory=\"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=42\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    directory=\"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=42\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6niI0LCpwmr",
        "outputId": "eb0f1b3e-7b30-4e06-af19-0e6ec2fccbf2"
      },
      "source": [
        "# 看一個epoch有多少個batch\n",
        "\n",
        "print(\n",
        "    \"Number of batches in raw_train_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_train_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_val_ds: %d\" % tf.data.experimental.cardinality(raw_val_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_test_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_test_ds)\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches in raw_train_ds: 313\n",
            "Number of batches in raw_val_ds: 79\n",
            "Number of batches in raw_test_ds: 391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnSmO94mqAev",
        "outputId": "e31c09e1-e95c-4058-8d45-9648b454e98c"
      },
      "source": [
        "# 看一個batch\n",
        "\n",
        "for text, label in raw_train_ds.take(count=1):\n",
        "    for i in range(5):\n",
        "        print(text.numpy()[i])         # tensor to numpy\n",
        "        print(label.numpy()[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"First of all, I liked very much the central idea of locating the '' intruders'', Others in the fragile Self, on various levels - mainly subconscious but sometimes more allegorical. In fact the intruders are omnipresent throughout the film : in the Swiss-French border where the pretagonist leads secluded life; in the his recurring daydream and nightmare; inside his ailing body after heart transplantation.... In the last half of the film, he becomes intruder himself, returning in ancient french colony in the hope of atoning for the past. <br /><br />The overall tone is bitter rather than pathetic, full of regrets and guilts, sense of failure being more or less dominant. This is a quite grim picture of an old age, ostensibly self-dependent but hopelessly void and lonely inside. The directer composes the images more to convey passing sensations of anxiety and desire than any explicit meanings. Some of them are mesmerizing, not devoid of humor though, kind of absurdist play only somnambulist can visualize.\"\n",
            "1\n",
            "b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "0\n",
            "b'After Harry Reems\\' teenage girlfriend is raped by Zebbedy Colt (The Night-Walker), Reems becomes despondent and consoles himself by having sex with some lesbians. Meanwhile, Colt, who carries a cane and dresses like a magician, rapes some more women. Eventually, Reems decides to track him down and end his crime spree. Despite being shot on film and marginally nasty, it looks like any other 70\\'s porno and is ineptly executed. The rape/abuse scenes are surprisingly restrained and the attempt to cash in on \"Death Wish\" is laughable. R. Bolla (\"Cannibal Holocaust\") plays a cop. Colt, who is usually over-the-top, wigs out in a couple of scenes, but he\\'s too well behaved for my money. This roughie could have been much rougher.'\n",
            "0\n",
            "b\"It's boggles the mind how this movie was nominated for seven Oscars and won one. Not because it's abysmal or because given the collective credentials of the creative team behind it really ought to deserve them but because in every category it was nominated Prizzi's Honor disappoints. Some would argue that old Hollywood pioneer John Huston had lost it by this point in his career but I don't buy it. Only the previous year he signed the superb UNDER THE VOLCANO, a dark character study set in Mexico, that ranks among the finest he ever did. Prizzi's Honor on the other hand, a film loaded with star power, good intentions and a decent script, proves to be a major letdown.<br /><br />The overall tone and plot of a gangster falling in love with a female hit-man prefigures the quirky crimedies that caught Hollywood by storm in the early 90's but the script is too convoluted for its own sake, the motivations are off and on the whole the story seems unsure of what exactly it's trying to be: a romantic comedy, a crime drama, a gangster saga etc. Jack Nicholson (doing a Brooklyn accent that works perfectly for De Niro but sounds unconvincing coming from Jack) and Kathleen Turner in the leading roles seem to be in paycheck mode, just going through the motions almost sleepwalking their way through some parts. Anjelica Huston on the other hand fares better but her performance is sabotaged by her character's motivations: she starts out the victim of her bigot father's disdain, she proves to be supportive to her ex-husband, then becomes a vindictive bitch that wants his head on a plate.<br /><br />The colours of the movie have a washed-up quality like it was made in the early 70's and Huston's direction is as uninteresting as everything else. There's promise behind the story and perhaps in the hands of a director hungry to be recognized it could've been morphed to something better but what's left looks like a film nobody was really interested in making.\"\n",
            "0\n",
            "b'This movie is pretty awful but I have some interesting information about it:<br /><br />It was filmed in 1976 at Northern Arizona University in Flagstaff, AZ, as well as at Oak Creek Canyon near Sedona, AZ. A good bulk of the extras in the film are then-drama students from NAU. I was a freshman there that year, minoring in theatre, but for some reason I didn\\'t get involved with the production. I did however know several people who did and can supply this rather odd fact:<br /><br />There is a scene in this movie where two of the principals, as part of their hazing ritual, have to run naked into the woods. They are seen from behind in the movie, doing just that. The thing is, those aren\\'t the actors at all but two guys I knew from the theatre department. The identity of these \"stunt posteriors\" will remain anonymous, at least to this website, unless they decide to, um, \"reveal\" themselves!'\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6xthHCvvTMx"
      },
      "source": [
        "> 可以發現有 br tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KCtUkjgPQEk"
      },
      "source": [
        "### 資料處理、準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WJKu7pmGQydL",
        "outputId": "2d14fa55-2aa9-4fd9-a28e-966901785b2b"
      },
      "source": [
        "import string\n",
        "\n",
        "string.punctuation"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdUa8LD_u4lg"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re\n",
        "\n",
        "def text_preprocessing(input_data):\n",
        "    input_data = tf.strings.lower(input=input_data)     # lower case\n",
        "    processed_input_data = tf.strings.regex_replace(    # 處理此tag\n",
        "        input=input_data,\n",
        "        pattern=\"<br />\",\n",
        "        rewrite=\" \"\n",
        "    )\n",
        "    return tf.strings.regex_replace(                    # 處理標點符號\n",
        "        input=processed_input_data,\n",
        "        pattern=\"[%s]\" % re.escape(string.punctuation),\n",
        "        rewrite=''\n",
        "    )\n",
        "\n",
        "# 文字處理設定, 用於TextVectorization\n",
        "max_tokens = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "\n",
        "# keras重點處理text的class\n",
        "vectorizer_layer = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    standardize=text_preprocessing,     # default是小寫+去除標點符號\n",
        "    output_mode='int',                  # 將token轉換成index表示, index 0留給masked token\n",
        "    output_sequence_length=sequence_length      # tokenzie最常長度\n",
        ")\n",
        "\n",
        "\n",
        "# 向量化\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)  # 先取出text only\n",
        "vectorizer_layer.adapt(text_ds)             # 相當於train的意思"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXoIFZSwTsHU"
      },
      "source": [
        "### 兩個方法去向量化文字資料\n",
        "- *成為模型的一部份*\n",
        "    - 實際使用上一條龍的處理會比較方便\n",
        "- 在資料集方面處理\n",
        "    - 此方法可以更好的使用CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr-2CqLVQZl1"
      },
      "source": [
        "# 法二\n",
        "\n",
        "def vectorizer_text(text, label):\n",
        "    text = tf.expand_dims(input=text, axis=-1)\n",
        "    return vectorizer_layer(text), label\n",
        "\n",
        "\n",
        "# 向量化\n",
        "train_ds = raw_train_ds.map(vectorizer_text)\n",
        "val_ds = raw_val_ds.map(vectorizer_text)\n",
        "test_ds = raw_test_ds.map(vectorizer_text)\n",
        "\n",
        "# 效能方面的trick, 透過快取\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_ds = train_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWmYX9TzXHLg"
      },
      "source": [
        "### 模型建立"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqdY5yI_WT97"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# 1. cnn-based, 透過functional API\n",
        "\n",
        "inputs = tf.keras.Input(shape=(sequence_length, ), dtype='int64')\n",
        "x = layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv1D(filters=128, kernel_size=7, padding='valid', activation='relu', strides=3)(x)\n",
        "x = layers.Conv1D(filters=128, kernel_size=7, padding='valid', activation='relu', strides=3)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)      # 功能同flattn\n",
        "x = layers.Dense(128, 'relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n",
        "\n",
        "cnn_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# compile\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1ekPOUjXp7w",
        "outputId": "ff1ba755-ac5c-4a5b-9718-89636bc93aff"
      },
      "source": [
        "cnn_model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 500)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 500, 128)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 165, 128)          114816    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 53, 128)           114816    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,806,273\n",
            "Trainable params: 2,806,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsViM2BNZvnc",
        "outputId": "7b3b1da0-56d7-4964-ed68-1fd5b203f0e1"
      },
      "source": [
        "# 訓練\n",
        "\n",
        "epochs = 5\n",
        "cnn_model.fit(\n",
        "    x=train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 49s 52ms/step - loss: 0.5974 - acc: 0.6112 - val_loss: 0.1875 - val_acc: 0.9357\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.2382 - acc: 0.9096 - val_loss: 0.0797 - val_acc: 0.9779\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.1246 - acc: 0.9566 - val_loss: 0.0531 - val_acc: 0.9822\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.0736 - acc: 0.9777 - val_loss: 0.0172 - val_acc: 0.9963\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.0458 - acc: 0.9836 - val_loss: 0.0179 - val_acc: 0.9970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f94064b49d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH4UZCK8aS0z",
        "outputId": "1d492ad9-052e-4bd4-f901-5bb65327536c"
      },
      "source": [
        "cnn_model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4187 - acc: 0.8612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.41873037815093994, 0.8611999750137329]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54BImtA5aII-",
        "outputId": "0d9c6fd1-7f4b-4ba6-cf6d-36a932597061"
      },
      "source": [
        "# predict出來是機率\n",
        "# p > 0.5 ---> class:1, otherwise: 0\n",
        "cnn_model.predict(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.57410717],\n",
              "       [0.39040452],\n",
              "       [0.98512137],\n",
              "       ...,\n",
              "       [0.10803606],\n",
              "       [0.9998933 ],\n",
              "       [0.01814001]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_P33yXpauqD",
        "outputId": "e298ef6a-8979-407f-c829-aff81ed4134e"
      },
      "source": [
        "for x, y in test_ds.take(count=1):\n",
        "    print(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  29  517  875 ... 1186    6    4]\n",
            " [   2  198 2443 ...    0    0    0]\n",
            " [  11   28  201 ...    0    0    0]\n",
            " ...\n",
            " [  45   22   25 ...    0    0    0]\n",
            " [4083 2890    7 ...    0    0    0]\n",
            " [  11   19    7 ...    0    0    0]], shape=(64, 500), dtype=int64) tf.Tensor(\n",
            "[1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1], shape=(64,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-wHDgclbavL"
      },
      "source": [
        "### RNN-basd model\n",
        "- 因為序列關係無法快速訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EehRsniDa34K"
      },
      "source": [
        "def create_model(model_type='rnn'):\n",
        "    \"\"\"\n",
        "        透過model_type去管控, 之後可以透過design pattern去優化。\n",
        "    \"\"\"\n",
        "    if model_type == 'cnn':\n",
        "        pass\n",
        "    elif model_type == 'rnn':\n",
        "        inputs = layers.Input(shape=(sequence_length, ), dtype='int64')\n",
        "        x = layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
        "        # x = layers.GRU(units=64, dropout=0.5, return_sequences=True, recurrent_dropout=0.2)(x)\n",
        "        x = layers.GRU(units=32, dropout=0.5, recurrent_dropout=0.2, activation='relu')(x)\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    else:\n",
        "        # transformer-based\n",
        "        pass\n",
        "    \n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5ddbxvGdhv1",
        "outputId": "75348b24-b8a8-4725-d53c-45eda5a1fa2b"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "rnn_model = create_model()\n",
        "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        [(None, 500)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_12 (Embedding)     (None, 500, 128)          2560000   \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 32)                15552     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,575,585\n",
            "Trainable params: 2,575,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnKNSFj_eL2f"
      },
      "source": [
        "# 太慢\n",
        "# rnn_model.fit(\n",
        "#     x=train_ds,\n",
        "#     validation_data=val_ds,\n",
        "#     epochs=epochs\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0lesF0DhnBB"
      },
      "source": [
        "> to be continued..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sph9uKgXgnFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d06489b-2b6f-4fe7-f718-1926edc74350"
      },
      "source": [
        "from transformer_block import TransformerBlock, TokenAndPositionEmbedding\n",
        "\n",
        "\n",
        "def create_model(num_transformers=6):\n",
        "    inputs = layers.Input(shape=(sequence_length,), dtype='int64')\n",
        "    x = TokenAndPositionEmbedding(maxlen=sequence_length, vocab_size=max_tokens, embed_dim=32)(inputs)\n",
        "    # 論文是6個\n",
        "    for _ in range(num_transformers):\n",
        "        x = TransformerBlock(embed_dim=32, num_heads=8, ff_dim=32)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)      # dimension reduction or flatten\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    output = layers.Dense(units=1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, output)\n",
        "    return model\n",
        "\n",
        "transformer_model = create_model()\n",
        "transformer_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 500)]             0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 500, 32)           656000    \n",
            "_________________________________________________________________\n",
            "transformer_block_8 (Transfo (None, 500, 32)           6464      \n",
            "_________________________________________________________________\n",
            "transformer_block_9 (Transfo (None, 500, 32)           6464      \n",
            "_________________________________________________________________\n",
            "transformer_block_10 (Transf (None, 500, 32)           6464      \n",
            "_________________________________________________________________\n",
            "transformer_block_11 (Transf (None, 500, 32)           6464      \n",
            "_________________________________________________________________\n",
            "transformer_block_12 (Transf (None, 500, 32)           6464      \n",
            "_________________________________________________________________\n",
            "transformer_block_13 (Transf (None, 500, 32)           6464      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 694,817\n",
            "Trainable params: 694,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrfUNtUIOmYg",
        "outputId": "9a165849-0c22-4cbe-e206-c91b167966d3"
      },
      "source": [
        "transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "transformer_model.fit(\n",
        "    x=train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 359s 1s/step - loss: 0.7157 - acc: 0.5249 - val_loss: 0.3757 - val_acc: 0.8520\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 346s 1s/step - loss: 0.3452 - acc: 0.8577 - val_loss: 0.1166 - val_acc: 0.9607\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 347s 1s/step - loss: 0.1751 - acc: 0.9376 - val_loss: 0.0466 - val_acc: 0.9862\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 347s 1s/step - loss: 0.0805 - acc: 0.9751 - val_loss: 0.0420 - val_acc: 0.9865\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 347s 1s/step - loss: 0.0556 - acc: 0.9833 - val_loss: 0.0276 - val_acc: 0.9926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5301ff97d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ag-9d_eQmHq",
        "outputId": "959952b8-d8f1-400a-d051-d3e10ea87771"
      },
      "source": [
        "transformer_model.evaluate(test_ds)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 114s 291ms/step - loss: 0.5669 - acc: 0.8374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5669289231300354, 0.8374000191688538]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwgpLan_QvEx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "文本分類-transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z8JX8YMGrAH"
      },
      "source": [
        "## 專案流程\n",
        "### - 資料讀取(keras built-in datasets)\n",
        "### - 資料處理(pad)\n",
        "### - 模型建立\n",
        "### - 超參數調整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO84gZDUHBoO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN_71ECBGjKV",
        "outputId": "f7642c07-a784-4e06-da37-7c48bf82cb6d"
      },
      "source": [
        "# 資料讀取\n",
        "vocab_size = 20000          # 只考慮20000個words\n",
        "max_len = 200               # 長度最多200\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=20000, maxlen=max_len)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14244,), (14244,), (14669,), (14669,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfWNSWhuHpGT",
        "outputId": "f93cefd6-3769-4c98-9cba-ba291c30cca4"
      },
      "source": [
        "# \n",
        "x_train[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 2,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 2,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 2,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 2,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQg7dXK9Huae",
        "outputId": "a0a0f395-7f16-43a9-95c9-76629bb527b5"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ceSzYLOIAIX"
      },
      "source": [
        "# 將其pad_sequence, 才可以送入model\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len, padding='post')\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len, padding='post')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD2ZMn0SIWvK",
        "outputId": "987ecbed-8a21-4b93-8509-0062d470df01"
      },
      "source": [
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14244, 200), (14669, 200))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wyhLgl3IYkt"
      },
      "source": [
        "# Encoder, PositionalEmbedding\n",
        "# 細節可以看Attention is all you need論文\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim=32, num_heads=8, ff_dim=32, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(units=ff_dim, activation='relu'),\n",
        "            layers.Dense(units=embed_dim, activation='relu')\n",
        "        ])\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        att_output = self.att(inputs, inputs, inputs)\n",
        "        att_output = self.drop1(att_output)\n",
        "        out1 = self.layer_norm1(att_output + inputs)    # short-cut\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.drop2(ffn_output)\n",
        "        out2 = self.layer_norm2(ffn_output + out1)      # short-cut\n",
        "        return out2\n",
        "\n",
        "\n",
        "class PositionalWordEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, max_len, embed_dim):\n",
        "        super(PositionalWordEmbedding, self).__init__()\n",
        "        self.pos_embedding = layers.Embedding(input_dim=max_len, output_dim=embed_dim, input_length=max_len)\n",
        "        self.word_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_len)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # 位置向量的init\n",
        "        max_len = tf.shape(inputs)[-1]      # 長度\n",
        "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
        "        positions = self.pos_embedding(positions)\n",
        "        word_embedding = self.word_embedding(inputs)\n",
        "\n",
        "        return positions + word_embedding"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZWpSPgTPKpe"
      },
      "source": [
        "[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6TFNZWUjEu"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, params):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = PositionalWordEmbedding(params['vocab_size'], params['max_len'], params['embed_dim'])\n",
        "        self.transformer_block = keras.Sequential([\n",
        "            TransformerBlock(params['embed_dim'], params['num_heads'], params['ff_dim'], params['rate']) for _ in range(params['layers'])\n",
        "        ])\n",
        "        self.flatten = layers.GlobalAveragePooling1D()\n",
        "        self.drop1 = layers.Dropout(params['rate'])\n",
        "        self.dense1 = layers.Dense(units=params['units'], activation='relu')\n",
        "        self.drop2 = layers.Dropout(params['rate'])\n",
        "        self.dense_out = layers.Dense(units=params['units_out'], activation=params['activation'])\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        inputs = self.transformer_block(inputs)\n",
        "        inputs = self.flatten(inputs)\n",
        "        inputs = self.drop1(inputs)\n",
        "        inputs = self.dense1(inputs)\n",
        "        inputs = self.drop2(inputs)\n",
        "        out = self.dense_out(inputs)\n",
        "        return out"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHyxs0dbNPrq",
        "outputId": "aabc823c-d52a-4606-d814-ac4dabb52a0f"
      },
      "source": [
        "## 超參數設定與訓練\n",
        "\n",
        "params = {\n",
        "    'max_len': 200,\n",
        "    'vocab_size': 20000,\n",
        "    'embed_dim': 32,\n",
        "    'num_heads': 2,\n",
        "    'ff_dim': 32,\n",
        "    'rate': 0.1,\n",
        "    'units': 16,\n",
        "    'units_out': 2,\n",
        "    'activation': 'softmax',\n",
        "    'layers': 1\n",
        "}\n",
        "\n",
        "\n",
        "model = Transformer(params)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "model.fit(x_train, y_train, 128, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "112/112 [==============================] - 7s 28ms/step - loss: 0.6541 - acc: 0.5839 - val_loss: 0.3285 - val_acc: 0.8577\n",
            "Epoch 2/5\n",
            "112/112 [==============================] - 3s 25ms/step - loss: 0.2655 - acc: 0.8946 - val_loss: 0.2805 - val_acc: 0.8852\n",
            "Epoch 3/5\n",
            "112/112 [==============================] - 3s 25ms/step - loss: 0.1518 - acc: 0.9493 - val_loss: 0.3313 - val_acc: 0.8788\n",
            "Epoch 4/5\n",
            "112/112 [==============================] - 3s 25ms/step - loss: 0.0884 - acc: 0.9734 - val_loss: 0.3836 - val_acc: 0.8718\n",
            "Epoch 5/5\n",
            "112/112 [==============================] - 3s 25ms/step - loss: 0.0560 - acc: 0.9843 - val_loss: 0.4929 - val_acc: 0.8671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f601cf1f450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmo6lpNnXgj-",
        "outputId": "12640978-566e-46bc-cca3-21c3236d1e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "positional_word_embedding_2  multiple                  646400    \n",
            "_________________________________________________________________\n",
            "sequential_17 (Sequential)   (None, 200, 32)           10656     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_8 ( multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             multiple                  528       \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             multiple                  34        \n",
            "=================================================================\n",
            "Total params: 657,618\n",
            "Trainable params: 657,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuTqmr_8Rr-2"
      },
      "source": [
        "# embed_dim = 32  # Embedding size for each token\n",
        "# num_heads = 2  # Number of attention heads\n",
        "# ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "# inputs = layers.Input(shape=(max_len,))\n",
        "# embedding_layer = PositionalWordEmbedding(vocab_size, max_len, embed_dim)\n",
        "# x = embedding_layer(inputs)\n",
        "# transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "# x = transformer_block(x)\n",
        "# x = layers.GlobalAveragePooling1D()(x)\n",
        "# x = layers.Dropout(0.1)(x)\n",
        "# x = layers.Dense(20, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.1)(x)\n",
        "# outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "# model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
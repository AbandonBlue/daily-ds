{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-text-extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpL8NY31gtpp"
      },
      "source": [
        "## BERT & Extration\n",
        "- [參考](https://keras.io/examples/nlp/text_extraction_with_bert/)\n",
        "\n",
        "### 介紹\n",
        "\n",
        "簡介\n",
        "- 這是一個QA task，用了論文最常、標準使用的SQuAD資料集，含有 question, paragraph for context。\n",
        "\n",
        "目標\n",
        "- 找到答案(span ---> start position, end postion)去回答Question，其中透過\"Exact Match\" 當作metrics去衡量模型效果。(對比於ground-truth, 有多少百分比是正確的)\n",
        "\n",
        "流程\n",
        "1. 將context and question 輸入 BERT模型\n",
        "2. 學習2個vector(S, T)相同維度於BERT的hidden state(才能計算相似性)\n",
        "3. 計算每一個token是start or end的機率，透過將S dot product BERT 最後一層的hidden state 去計算softmax。然而T也是相同，只是代表的是end。\n",
        "4. Fine-tune BERT and learn S and T along the way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBLL7aS0jdBA",
        "outputId": "774e20d9-273c-46d2-c79d-dd5180ced316"
      },
      "source": [
        "!pip install tokenizers transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giOjbD6qgfny"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBNxXlncg6oh"
      },
      "source": [
        "max_len = 384\n",
        "config = BertConfig()       # default 是論文參數"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dURwqdKpj0fF"
      },
      "source": [
        "### BERT tokenzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNukDCgJjwKX"
      },
      "source": [
        "slow_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "save_path = 'bert_bast_uncased/'\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# 重新load, 快速版本\n",
        "tokenizer = BertWordPieceTokenizer(vocab=save_path + 'vocab.txt', lowercase=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIzDCZztkqep"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwbIA7sCkltI"
      },
      "source": [
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMEPCM6ak10S"
      },
      "source": [
        "### 處理資料\n",
        "- 將json檔案儲存到物件中\n",
        "- 透過物件建立x_train, y_train, x_eval, y_eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hvSEn3Zk8dx"
      },
      "source": [
        "# 巢狀 dict\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL2sp9c7lVuC",
        "outputId": "0fec67de-5a8d-4316-b0df-3a391c673686"
      },
      "source": [
        "len(raw_train_data['data']), len(raw_train_data['data'][0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIYjkAiPlXlA",
        "outputId": "15ce3656-7514-40c7-ac3b-986b61efa00e"
      },
      "source": [
        "raw_train_data.keys()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'version'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyi_kG9NmBib",
        "outputId": "ba71ce31-9ada-404f-8692-d7f26c08acbb"
      },
      "source": [
        "raw_train_data['data'][0].keys()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['title', 'paragraphs'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gAnj7AdenWbp",
        "outputId": "183a96d2-f331-472a-99e7-ff017b2e9001"
      },
      "source": [
        "raw_train_data['data'][0]['title']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'University_of_Notre_Dame'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTMkiicmngwx",
        "outputId": "d2f5b1d4-dc25-49eb-8c6a-7e27c209d430"
      },
      "source": [
        "len(raw_train_data['data'][0]['paragraphs']), raw_train_data['data'][0]['paragraphs'][0].keys()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55, dict_keys(['context', 'qas']))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "ruxaxyC7nru_",
        "outputId": "62fcd837-616d-4778-864a-dec800afc666"
      },
      "source": [
        "raw_train_data['data'][0]['paragraphs'][0]['context']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX_k49P8n2_6",
        "outputId": "1b1d044b-fd87-47a8-c230-5a2eed3d488a"
      },
      "source": [
        "raw_train_data['data'][0]['paragraphs'][0]['qas']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answers': [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}],\n",
              "  'id': '5733be284776f41900661182',\n",
              "  'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'},\n",
              " {'answers': [{'answer_start': 188, 'text': 'a copper statue of Christ'}],\n",
              "  'id': '5733be284776f4190066117f',\n",
              "  'question': 'What is in front of the Notre Dame Main Building?'},\n",
              " {'answers': [{'answer_start': 279, 'text': 'the Main Building'}],\n",
              "  'id': '5733be284776f41900661180',\n",
              "  'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?'},\n",
              " {'answers': [{'answer_start': 381,\n",
              "    'text': 'a Marian place of prayer and reflection'}],\n",
              "  'id': '5733be284776f41900661181',\n",
              "  'question': 'What is the Grotto at Notre Dame?'},\n",
              " {'answers': [{'answer_start': 92,\n",
              "    'text': 'a golden statue of the Virgin Mary'}],\n",
              "  'id': '5733be284776f4190066117e',\n",
              "  'question': 'What sits on top of the Main Building at Notre Dame?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4LUPH4Hqx9c",
        "outputId": "8ad8fd1e-348a-40fc-868a-4468d99762c0"
      },
      "source": [
        "help(tokenizer.encode)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method encode in module tokenizers.implementations.base_tokenizer:\n",
            "\n",
            "encode(sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding method of tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer instance\n",
            "    Encode the given sequence and pair. This method can process raw text sequences as well\n",
            "    as already pre-tokenized sequences.\n",
            "    \n",
            "    Args:\n",
            "        sequence: InputSequence:\n",
            "            The sequence we want to encode. This sequence can be either raw text or\n",
            "            pre-tokenized, according to the `is_pretokenized` argument:\n",
            "    \n",
            "            - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n",
            "            - If `is_pretokenized=True`: `InputSequence` is expected to be\n",
            "                `Union[List[str], Tuple[str]]`\n",
            "    \n",
            "        is_pretokenized: bool:\n",
            "            Whether the input is already pre-tokenized.\n",
            "    \n",
            "        add_special_tokens: bool:\n",
            "            Whether to add the special tokens while encoding.\n",
            "    \n",
            "    Returns:\n",
            "        An Encoding\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAdeHDnqq9Yi",
        "outputId": "fae0a6b2-d89d-4688-9639-b4c9926ac54b"
      },
      "source": [
        "# tokenizer 使用說明\n",
        "\n",
        "# 將str轉換成encoding\n",
        "encoding = tokenizer.encode('Aaron will be one of the best data scientist!')\n",
        "print(encoding)\n",
        "\n",
        "# 透過attrubute 讀取需要的資訊\n",
        "encoding.ids"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 7158, 2097, 2022, 2028, 1997, 1996, 2190, 2951, 7155, 999, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM7MgwE1vy-A",
        "outputId": "5ea31b9f-a2de-499d-c7d3-7df19a13f8c3"
      },
      "source": [
        "# sub-tokens: What is offset in tokenizer? For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token's start position and end position relative to the original token it was split from. That means that if the first position in the tuple is anything other than 0, we will set its corresponding label to -100\n",
        "\n",
        "encoding.offsets"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (0, 5),\n",
              " (6, 10),\n",
              " (11, 13),\n",
              " (14, 17),\n",
              " (18, 20),\n",
              " (21, 24),\n",
              " (25, 29),\n",
              " (30, 34),\n",
              " (35, 44),\n",
              " (44, 45),\n",
              " (0, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5AEjdelmy0VQ",
        "outputId": "c1d97355-8d9a-422c-a0bf-fd2d8221b8d9"
      },
      "source": [
        "tokenizer.id_to_token(101)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS]'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYM4BDY3yRR9",
        "outputId": "cb649cbe-6a65-43e4-ff84-edba240504f3"
      },
      "source": [
        "# 要跳過第一個ids, 因為第一個是[CLS]\n",
        "print(raw_train_data['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
        "tokenizer.encode(raw_train_data['data'][0]['paragraphs'][0]['qas'][0]['question']).ids"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 2000,\n",
              " 3183,\n",
              " 2106,\n",
              " 1996,\n",
              " 6261,\n",
              " 2984,\n",
              " 9382,\n",
              " 3711,\n",
              " 1999,\n",
              " 8517,\n",
              " 1999,\n",
              " 10223,\n",
              " 26371,\n",
              " 2605,\n",
              " 1029,\n",
              " 102]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPURFksZrp32"
      },
      "source": [
        "# SquadExample class: 讀取基本單位組合成可用資料型態\n",
        "\n",
        "class SquadExample():\n",
        "    def __init__( self, question, context, start_idx, answer_text, all_answers):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_idx = start_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False       # *代表此資料無法使用, 跳過\n",
        "    \n",
        "    def preprocess(self):\n",
        "        # 處理 context, answer, question\n",
        "        context = ' '.join(str(self.context).split())\n",
        "        question = ' '.join(str(self.question).split())\n",
        "        answer = ' '.join(str(self.answer_text).split())\n",
        "\n",
        "        # 找到end idx\n",
        "        end_idx = self.start_idx + len(answer)\n",
        "        if end_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return      # 不繼續處理\n",
        "        \n",
        "        # 生成一個boolean list to 代表是否為答案: 0-> 不相關, 1-> 相關答案\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(self.start_idx, end_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "        \n",
        "        # tokenize the context\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "        # *Find tokens that were created from answer characters\n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "        \n",
        "        # 如果沒有答案, 跳過\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "        \n",
        "        # 找到start以及end的token idx\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # tokenize the question\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "        # 將資料轉換成BERT可輸入形式\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]      # 跳過[CLS], 只要context有即可。\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks\n",
        "        # 如果需要truncation, skip\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:\n",
        "            input_ids += [0] * padding_length\n",
        "            attention_mask += [0] * padding_length\n",
        "            token_type_ids += [0] * padding_length      # 因為attention_mask 是否為1沒有差\n",
        "        elif padding_length < 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "        \n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSCuJA_n5RY"
      },
      "source": [
        "def create_squad_examples(raw_data):\n",
        "    \"\"\"\n",
        "        將讀入的dict轉換成SquasExample物件並處理\n",
        "    \"\"\"\n",
        "    squad_examples = []\n",
        "    for item in raw_data['data']:\n",
        "        for para in item['paragraphs']:\n",
        "            # 上下文只有一則\n",
        "            # 但qa有多個\n",
        "            context = para['context']\n",
        "            for qa in para['qas']:\n",
        "                question = qa['question']\n",
        "                answer_text = qa['answers'][0]['text']\n",
        "                all_answers = [_['text'] for _ in qa['answers']]     # answer_text 的集合\n",
        "                start_idx = qa['answers'][0]['answer_start']\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_idx, answer_text, all_answers\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples: list):\n",
        "    \"\"\"\n",
        "        將SquadExample物件 讀取並得到標準輸入\n",
        "    \"\"\"\n",
        "    dataset_dict = {\n",
        "        'input_ids': [],\n",
        "        'token_type_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'start_token_idx': [],\n",
        "        'end_token_idx': []\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))        # getattr\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])             # 將list 轉型 ndarray, why?下面又變成list了呀...\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict['start_token_idx'], dataset_dict['end_token_idx']]\n",
        "    return x, y"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXSGTm__V0uj",
        "outputId": "9cc71e67-d175-4433-d6b5-7d9b947d1048"
      },
      "source": [
        "# 取得資料\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87599 training points created.\n",
            "10570 evaluation points created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHZiobs1Xaga"
      },
      "source": [
        "### 透過 Keras Functional API 建造 QA-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfuM6kcOV7ox"
      },
      "source": [
        "# encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def create_model():\n",
        "    # BERT Encoder\n",
        "    encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # QA Model\n",
        "    input_ids = layers.Input(shape=(max_len, ), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0] # [0] ---> [CLS]\n",
        "\n",
        "    print(embedding.shape)\n",
        "\n",
        "    start_logits = layers.Dense(1, name='start_logit')(embedding)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name='end_logit')(embedding)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation('softmax')(start_logits)\n",
        "    end_probs = layers.Activation('softmax')(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs]\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWnjEgclYxsT",
        "outputId": "77d88775-cd28-4834-8beb-9496762c8f73"
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.53.184.202:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.53.184.202:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5e487b2f30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5e487b2f30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5e487b2f30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f5e63967950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f5e63967950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f5e63967950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 384, 768)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       769         tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       769         tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 384)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6FZS7KZb6hf"
      },
      "source": [
        "### Create evaluation Callback\n",
        "This callback will compute the exact match score using the validation data after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGQWXdPWbRFL"
      },
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Remove articles\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Each `SquadExample` object contains the character level offsets for each token\n",
        "    in its input paragraph. We use them to get back the span of text corresponding\n",
        "    to the tokens between our predicted start and end tokens.\n",
        "    All the ground-truth answers are also present in each `SquadExample` object.\n",
        "    We calculate the percentage of data points where the span of text obtained\n",
        "    from model predictions matches one of the ground-truth answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)      # 機率\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)        # 取得 token idx\n",
        "            end = np.argmax(end)            # ..\n",
        "            if start >= len(offsets):       # 無效的情況\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImODlnCzb_yg",
        "outputId": "dc4b4325-3912-48e1-f99a-1691eae77800"
      },
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=3,  # For demonstration, 3 epochs are recommended\n",
        "    batch_size=64,\n",
        "    callbacks=[exact_match_callback],\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None,) dtype=int64>, <tf.Tensor 'cond_8/Identity_4:0' shape=(None,) dtype=int64>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None,) dtype=int64>, <tf.Tensor 'cond_8/Identity_4:0' shape=(None,) dtype=int64>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   6/1346 [..............................] - ETA: 4:27 - loss: 10.6494 - activation_loss: 5.4385 - activation_1_loss: 5.2110WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 9.1781s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 9.1781s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1346/1346 [==============================] - 408s 238ms/step - loss: 2.5503 - activation_loss: 1.3342 - activation_1_loss: 1.2161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 384) dtype=int64>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 384) dtype=int64>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch=1, exact match score=0.78\n",
            "Epoch 2/3\n",
            "1346/1346 [==============================] - 268s 199ms/step - loss: 1.5512 - activation_loss: 0.8265 - activation_1_loss: 0.7248\n",
            "\n",
            "epoch=2, exact match score=0.78\n",
            "Epoch 3/3\n",
            "1346/1346 [==============================] - 269s 199ms/step - loss: 1.0928 - activation_loss: 0.5884 - activation_1_loss: 0.5044\n",
            "\n",
            "epoch=3, exact match score=0.78\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5cd4569ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg9DfBvzcBrD"
      },
      "source": [
        "# 預測\n",
        "\n",
        "y_pred = model.predict(x_eval)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItW8u9pvjBSw",
        "outputId": "d5271d37-cc73-4d79-9bed-e88a8ba12d2e"
      },
      "source": [
        "y_pred[0],  y_pred[1]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.03202388e-09, 2.59929675e-06, 7.31851131e-08, ...,\n",
              "         3.11184650e-12, 3.36613550e-12, 3.41531534e-12],\n",
              "        [4.17690593e-09, 2.05314427e-06, 1.29428358e-07, ...,\n",
              "         3.01536435e-11, 3.26477456e-11, 3.27239659e-11],\n",
              "        [7.98829296e-06, 9.15473502e-05, 7.36842776e-06, ...,\n",
              "         3.75526010e-09, 3.76080012e-09, 3.71986997e-09],\n",
              "        ...,\n",
              "        [1.12251115e-07, 1.03875098e-03, 2.56458158e-03, ...,\n",
              "         7.53170026e-11, 7.43409292e-11, 7.36495587e-11],\n",
              "        [5.97524590e-08, 1.33132766e-04, 9.72560432e-04, ...,\n",
              "         3.63567343e-10, 3.61021740e-10, 3.49573120e-10],\n",
              "        [1.26235648e-07, 3.92975993e-02, 6.64244145e-02, ...,\n",
              "         4.33438840e-10, 4.32379660e-10, 4.27838320e-10]], dtype=float32),\n",
              " array([[1.55209556e-09, 6.03247997e-07, 1.68573052e-07, ...,\n",
              "         2.08076091e-11, 2.06560099e-11, 1.95716811e-11],\n",
              "        [3.34784978e-09, 4.01949364e-07, 1.36644644e-07, ...,\n",
              "         1.83916535e-10, 1.85570184e-10, 1.76207091e-10],\n",
              "        [2.66556208e-06, 1.36093922e-06, 6.31932721e-07, ...,\n",
              "         1.19990666e-08, 1.18893313e-08, 1.13841327e-08],\n",
              "        ...,\n",
              "        [5.54021184e-09, 1.71640731e-05, 1.86876990e-04, ...,\n",
              "         1.89232380e-11, 1.94526530e-11, 1.93668432e-11],\n",
              "        [3.87601817e-09, 1.97768077e-06, 4.63342876e-05, ...,\n",
              "         1.04653612e-10, 1.05472776e-10, 1.07354049e-10],\n",
              "        [1.03576934e-08, 2.67419236e-04, 3.87399504e-03, ...,\n",
              "         1.13905559e-10, 1.18471288e-10, 1.16970350e-10]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Kv7ozFjPCh",
        "outputId": "46e851f8-22f2-4cac-b0c2-97d50eb3773f"
      },
      "source": [
        "# start 機率\n",
        "y_pred[0].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10331, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvxnM66tjWhe",
        "outputId": "ba7a2654-1261-4bd1-9ed2-f4d2b9198809"
      },
      "source": [
        "# end 機率\n",
        "y_pred[1].shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10331, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfoLx6WbjYry",
        "outputId": "902288a6-ded5-48f1-95b4-23d560eab34f"
      },
      "source": [
        "# 抽出一個驗證\n",
        "\n",
        "print(sum(y_pred[0][0]))\n",
        "print(sum(y_pred[1][0]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.000000259787777\n",
            "1.0000000447016262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZXnn0ckjmfs",
        "outputId": "ba274173-8428-41e8-e646-098f647d335b"
      },
      "source": [
        "# 直接表示, 使用最後一個輸入來看看效果\n",
        "\n",
        "\n",
        "\n",
        "pred_start, pred_end = model.predict(x_eval)      # 機率\n",
        "count = 0\n",
        "eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    squad_eg = eval_examples_no_skip[idx]\n",
        "    offsets = squad_eg.context_token_to_char\n",
        "    start = np.argmax(start)        # 取得 token idx\n",
        "    end = np.argmax(end)            # ..\n",
        "    if start >= len(offsets):       # 無效的情況\n",
        "        continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "        pred_char_end = offsets[end][1]\n",
        "        pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "        pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "    normalized_pred_ans = normalize_text(pred_ans)\n",
        "    normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "    if normalized_pred_ans in normalized_true_ans:\n",
        "        count += 1\n",
        "acc = count / len(y_eval[0])\n",
        "print(f\"exact match score={acc:.2f}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exact match score=0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3dyuRvUl0O0",
        "outputId": "4db5a73b-20f1-4214-dc72-fa7d79a45d63"
      },
      "source": [
        "# 上下文\n",
        "print(squad_eg.context)\n",
        "# 問題\n",
        "print(squad_eg.question)\n",
        "# 答案\n",
        "print(squad_eg.answer_text)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at 1 m·s−2 when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.\n",
            "What is the seldom used force unit equal to one thousand newtons?\n",
            "sthène\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X9svWR8mTxw"
      },
      "source": [
        "# 整理\n",
        "\n",
        "def look():\n",
        "    squad_egs, pred_text_answers = [], []\n",
        "    pred_start, pred_end = model.predict(x_eval)      # 機率\n",
        "    count = 0\n",
        "    eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "        squad_eg = eval_examples_no_skip[idx]\n",
        "        offsets = squad_eg.context_token_to_char\n",
        "        start = np.argmax(start)        # 取得 token idx\n",
        "        end = np.argmax(end)            # ..\n",
        "        if start >= len(offsets):       # 無效的情況\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "        else:\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "        normalized_pred_ans = normalize_text(pred_ans)\n",
        "        normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "        if normalized_pred_ans in normalized_true_ans:\n",
        "            count += 1\n",
        "        \n",
        "        # 儲存\n",
        "        squad_egs.append(squad_eg)\n",
        "        pred_text_answers.append(normalized_pred_ans)\n",
        "    acc = count / len(y_eval[0])\n",
        "\n",
        "    return squad_egs, pred_text_answers\n",
        "    print(f\"exact match score={acc:.2f}\")\n",
        "\n",
        "squad_egs, pred_text_answers = look()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct6E3t40nPr5",
        "outputId": "78089f0e-e0b1-4819-8100-a5903f14789e"
      },
      "source": [
        "for i in range(10):\n",
        "    squad_eg = squad_egs[i]\n",
        "    print(f'第 {i} 個樣本')\n",
        "    # 上下文\n",
        "    print(f'上下文: {squad_eg.context}')\n",
        "    # 問題\n",
        "    print(f'問題: {squad_eg.question}')\n",
        "    # 答案\n",
        "    print(f'答案: {squad_eg.answer_text}')\n",
        "    # 預測結果\n",
        "    print(f'預測結果: {pred_text_answers[i]}')\n",
        "    print('-'*30)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第 0 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: Which NFL team represented the AFC at Super Bowl 50?\n",
            "答案: Denver Broncos\n",
            "預測結果: denver broncos\n",
            "------------------------------\n",
            "第 1 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: Which NFL team represented the NFC at Super Bowl 50?\n",
            "答案: Carolina Panthers\n",
            "預測結果: carolina panthers\n",
            "------------------------------\n",
            "第 2 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: Where did Super Bowl 50 take place?\n",
            "答案: Santa Clara, California\n",
            "預測結果: levis stadium\n",
            "------------------------------\n",
            "第 3 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: Which NFL team won Super Bowl 50?\n",
            "答案: Denver Broncos\n",
            "預測結果: denver broncos\n",
            "------------------------------\n",
            "第 4 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
            "答案: gold\n",
            "預測結果: gold\n",
            "------------------------------\n",
            "第 5 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: What was the theme of Super Bowl 50?\n",
            "答案: \"golden anniversary\"\n",
            "預測結果: golden anniversary\n",
            "------------------------------\n",
            "第 6 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: What day was the game played on?\n",
            "答案: February 7, 2016\n",
            "預測結果: february 7 2016\n",
            "------------------------------\n",
            "第 7 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: What is the AFC short for?\n",
            "答案: American Football Conference\n",
            "預測結果: american football conference\n",
            "------------------------------\n",
            "第 8 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: What was the theme of Super Bowl 50?\n",
            "答案: \"golden anniversary\"\n",
            "預測結果: golden anniversary\n",
            "------------------------------\n",
            "第 9 個樣本\n",
            "上下文: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "問題: What does AFC stand for?\n",
            "答案: American Football Conference\n",
            "預測結果: american football conference\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNo7kJD-mSgd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
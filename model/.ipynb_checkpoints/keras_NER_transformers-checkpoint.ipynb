{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4dH8XV99QV1"
   },
   "source": [
    "## 介紹\n",
    "- NER: 命名實體識別，對於以 word 為單位會給予更細緻的資訊，在一些特定可以產生大用。[參考資訊](https://medium.com/royes-researchcraft/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-3-%E5%91%BD%E5%90%8D%E5%AF%A6%E9%AB%94%E6%A8%99%E8%A8%BB-name-entity-recognition-%E7%90%86%E8%AB%96%E8%A8%AD%E8%A8%88%E7%AF%87-923348c31a7b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Gu8LRcD9CYz",
    "outputId": "a90c54a3-1fd4-4625-a60a-6dc99cda4bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.11.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.16)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "--2021-09-06 02:35:35--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7502 (7.3K) [text/plain]\n",
      "Saving to: ‘conlleval.py.1’\n",
      "\n",
      "conlleval.py.1      100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-06 02:35:35 (40.7 MB/s) - ‘conlleval.py.1’ saved [7502/7502]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取得資料集\n",
    "\n",
    "!pip install datasets\n",
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aHZXs4A2-woL"
   },
   "outputs": [],
   "source": [
    "# 模組\n",
    "\n",
    "import os\n",
    "import numpy as np  \n",
    "from datasets import load_dataset       # HuggingFace datasets\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import Counter\n",
    "from conlleval import evaluate      # *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f6It5eDz_OQZ"
   },
   "outputs": [],
   "source": [
    "# 先從模型建構開始\n",
    "# Transformer block, 需要encoder部分\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(units=ff_dim, activation='relu'),\n",
    "            layers.Dense(units=embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attention_output = self.att(inputs, inputs)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y-CbJtAABK-u"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bkj48CmhCWEK"
   },
   "outputs": [],
   "source": [
    "# NER model\n",
    "\n",
    "class NERTransformer(keras.Model):\n",
    "    def __init__(self, num_tags, maxlen=128, vocab_size=30000, embed_dim=32, num_heads=2, ff_dim=32, rate=0.1):\n",
    "        super(NERTransformer, self).__init__()\n",
    "        self.embedding = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(embed_dim, num_heads, ff_dim, rate)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.ffn = layers.Dense(ff_dim, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.classifier = layers.Dense(units=num_tags, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Apk8C4-2EFak",
    "outputId": "8de886aa-6eff-43c2-c166-236092a03a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "ner_transformer (NERTransfor (None, 128, 10)           976138    \n",
      "=================================================================\n",
      "Total params: 976,138\n",
      "Trainable params: 976,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    inputs = keras.Input(shape=(128, ), dtype=tf.int32)\n",
    "    outputs = NERTransformer(num_tags=10)(inputs)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "ner_model = create_model()\n",
    "ner_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2DiuPOm5ooR",
    "outputId": "9d1e6e8c-7bff-44f4-a5ea-2015175df9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128, 10), dtype=float32, numpy=\n",
       "array([[[0.06237963, 0.09621482, 0.02808117, ..., 0.18108463,\n",
       "         0.07764017, 0.06209963],\n",
       "        [0.13527536, 0.02988676, 0.10768166, ..., 0.2491994 ,\n",
       "         0.10046712, 0.04631824],\n",
       "        [0.17903793, 0.12856357, 0.11218224, ..., 0.23698992,\n",
       "         0.07657179, 0.02472914],\n",
       "        ...,\n",
       "        [0.18590496, 0.13187802, 0.00812685, ..., 0.29874146,\n",
       "         0.22643621, 0.00977837],\n",
       "        [0.18811896, 0.08036596, 0.00589105, ..., 0.07163271,\n",
       "         0.20709743, 0.06145992],\n",
       "        [0.04475464, 0.07802353, 0.01901839, ..., 0.19102167,\n",
       "         0.30132622, 0.05419953]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(np.random.randint(2, 100, size=(1,128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_b1BZ2YvJW4"
   },
   "source": [
    "## Load the CoNLL 2003 dataset from the datasets library and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzZCXkrDEYu0",
    "outputId": "4555f095-bbcf-4cc8-9ea1-b6180d0acdcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_data = load_dataset(\"conll2003\")\n",
    "conll_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdNwMmzXvf55",
    "outputId": "a39aabdc-4175-4484-f0f9-b2b69d04faee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GUWLTACvxLy",
    "outputId": "41384c09-ced4-437c-d20c-253bb9105e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22'], ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.'], ['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'], ['\"', 'We', 'do', \"n't\", 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', \"n't\", 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'Commission', \"'s\", 'chief', 'spokesman', 'Nikolaus', 'van', 'der', 'Pas', 'told', 'a', 'news', 'briefing', '.'], ['He', 'said', 'further', 'scientific', 'study', 'was', 'required', 'and', 'if', 'it', 'was', 'found', 'that', 'action', 'was', 'needed', 'it', 'should', 'be', 'taken', 'by', 'the', 'European', 'Union', '.'], ['He', 'said', 'a', 'proposal', 'last', 'month', 'by', 'EU', 'Farm', 'Commissioner', 'Franz', 'Fischler', 'to', 'ban', 'sheep', 'brains', ',', 'spleens', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'precautionary', 'move', 'to', 'protect', 'human', 'health', '.'], ['Fischler', 'proposed', 'EU-wide', 'measures', 'after', 'reports', 'from', 'Britain', 'and', 'France', 'that', 'under', 'laboratory', 'conditions', 'sheep', 'could', 'contract', 'Bovine', 'Spongiform', 'Encephalopathy', '(', 'BSE', ')', '--', 'mad', 'cow', 'disease', '.'], ['But', 'Fischler', 'agreed', 'to', 'review', 'his', 'proposal', 'after', 'the', 'EU', \"'s\", 'standing', 'veterinary', 'committee', ',', 'mational', 'animal', 'health', 'officials', ',', 'questioned', 'if', 'such', 'action', 'was', 'justified', 'as', 'there', 'was', 'only', 'a', 'slight', 'risk', 'to', 'human', 'health', '.']]\n",
      "[[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0], [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 7, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(conll_data['train']['tokens'][:10])\n",
    "print(conll_data['train']['ner_tags'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XWpNs83tvQCO"
   },
   "outputs": [],
   "source": [
    "# 將其轉換為tf.data.Dataset\n",
    "# len   sentence of tokens sentence of ner_tags\n",
    "\n",
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            tokens = record[\"tokens\"]\n",
    "            f.write(\n",
    "                str(len(tokens))\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(tokens)\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(map(str, ner_tags))\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "os.mkdir(\"data\")\n",
    "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5J1deMadw6y2",
    "outputId": "82f04228-d783-4eb0-8ac2-fc885f57e287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "# 查詢table\n",
    "\n",
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYW6YBl-xXjE",
    "outputId": "40a7ecf1-69e3-43ff-a08f-2f679600b4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21009\n"
     ]
    }
   ],
   "source": [
    "# all_tokens = []\n",
    "# for sub_tokens in conll_data['train']['tokens']:\n",
    "#     all_tokens += sub_tokens\n",
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))       # 小寫所有token\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))            \n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = len(counter)\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(\n",
    "    vocabulary=vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAuRRn__yFCH",
    "outputId": "00f559f7-e50b-4919-ef6e-00145f55b668"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 8390),\n",
       " ('.', 7374),\n",
       " (',', 7290),\n",
       " ('of', 3815),\n",
       " ('in', 3621),\n",
       " ('to', 3424),\n",
       " ('a', 3199),\n",
       " ('and', 2872),\n",
       " ('(', 2861),\n",
       " (')', 2861)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xsuu31_8ygmH"
   },
   "outputs": [],
   "source": [
    "# 創造 tf.data.Dataset\n",
    "# a line = a row data\n",
    "\n",
    "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Yladg4Nyv7I",
    "outputId": "255b8a39-55f4-4a6a-bf13-28761e8291a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x in train_data:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jf0ycevPyzJs",
    "outputId": "66d41c7e-f535-4644-d54b-f78e4d000c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VaIUMNj-0NXM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HaLWKiXuy9Hz"
   },
   "outputs": [],
   "source": [
    "# 將dataset, 透過函數轉換成可使用的input\n",
    "\n",
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(input=record, sep='\\t')\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32) # 此位置為 length\n",
    "    tokens = record[1:length+1]     \n",
    "    tags = record[length+1:]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)        # *\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "\n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QzWx6kuL0RQ-"
   },
   "outputs": [],
   "source": [
    "# We use `padded_batch` here because each record in the dataset has a\n",
    "# different length.\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "# ner_model = create_model(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "ner_model = NERTransformer(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XJ1b7alw0ke6"
   },
   "outputs": [],
   "source": [
    "# 透過自制loss去忽略padding計算, 用sample_weight也可以做到。\n",
    "\n",
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name='custom_ner_loss'):\n",
    "        super(CustomNonPaddingTokenLoss, self).__init__(name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "            reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast(x=(y_true > 0), dtype=tf.float32)        # 轉型\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VH9Fiihg11Zo",
    "outputId": "e2f84065-37fb-48bd-98a3-4e5a71c05379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 10s 18ms/step - loss: 0.6240\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.2510\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.1483\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.1127\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.0903\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.0739\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.0630\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.0548\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.0498\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 8s 18ms/step - loss: 0.0442\n",
      "tf.Tensor([[  988 10950   204   628     6  3938   215  5773]], shape=(1, 8), dtype=int64)\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n"
     ]
    }
   ],
   "source": [
    "# 模型設定以及訓練\n",
    "\n",
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)\n",
    "\n",
    "\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "\n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"eu rejects german call to boycott british lamb\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTN8BQmp17m0",
    "outputId": "9df790a2-e5fd-429e-c217-3cd8494e55de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 8790 phrases; correct: 4113.\n",
      "accuracy:  66.10%; (non-O)\n",
      "accuracy:  89.40%; precision:  46.79%; recall:  69.22%; FB1:  55.84\n",
      "              LOC: precision:  50.95%; recall:  81.93%; FB1:  62.83  2954\n",
      "             MISC: precision:  55.34%; recall:  68.00%; FB1:  61.02  1133\n",
      "              ORG: precision:  30.57%; recall:  69.80%; FB1:  42.52  3062\n",
      "              PER: precision:  63.68%; recall:  56.73%; FB1:  60.01  1641\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    evaluate(real_tags, predicted_tags)\n",
    "\n",
    "\n",
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "T8amkzGf36-g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "keras-NER-transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras實現BERT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeTOr0Mu8cpT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMcSZpoY-KLg",
        "outputId": "edc6d9f7-8806-43e5-a739-60a5513f6328"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 128\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 12\n",
        "\n",
        "\n",
        "config = Config()\n",
        "config"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NdzwQCe1Esp",
        "outputId": "5000a83d-090a-459c-bddd-3339c0c19af5"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz        # 下載\n",
        "!tar -xf aclImdb_v1.tar.gz                                                      # 解壓縮"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  14.3M      0  0:00:05  0:00:05 --:--:-- 18.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvBA7myo-Ugh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7ea709-afca-4c7a-e0e4-1fe57e96fa8e"
      },
      "source": [
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name) as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "    pos_files = glob.glob('aclImdb/' + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob('aclImdb/' + folder_name + '/neg/*.txt')\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            'review': pos_texts + neg_texts,\n",
        "            'sentiment': [0] * len(pos_texts) + [1] * len(neg_texts)\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)      # shuffle + reset_index\n",
        "    return df\n",
        "\n",
        "train_df = get_data_from_text_files('train')\n",
        "test_df = get_data_from_text_files('test')\n",
        "\n",
        "all_data = train_df.append(other=test_df)               # df.append(other): 直接append到後面 \n",
        "all_data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwydIntaDIwe"
      },
      "source": [
        "def custom_standarization(input_data):\n",
        "    \"\"\"\n",
        "        客製化的處理text資料, 用於text layer。\n",
        "        處理:\n",
        "            小寫、去除html tag、去除標點符號\n",
        "    \"\"\"\n",
        "    lowercase = tf.strings.lower(input=input_data)\n",
        "    stripped_html = tf.strings.regex_replace(\n",
        "        input=lowercase,\n",
        "        pattern=\"<br />\",\n",
        "        rewrite=' '\n",
        "    )\n",
        "    return tf.strings.regex_replace(\n",
        "        input=stripped_html,\n",
        "        pattern=re.escape(pattern=\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"),\n",
        "        rewrite=''\n",
        "    )\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"MASK\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorizer_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        standardize=custom_standarization,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=max_seq\n",
        "    )\n",
        "    vectorizer_layer.adapt(texts)           # fit\n",
        "\n",
        "    # 插入mask token\n",
        "    vocab = vectorizer_layer.get_vocabulary()   # 30000\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]    # 0:2 ---> '', '[UNK]', 不要這兩個why?\n",
        "    vectorizer_layer.set_vocabulary(vocab)      # 29998\n",
        "    return vectorizer_layer\n",
        "\n",
        "\n",
        "# 文字處理layer init\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCs9wYMR4IPG",
        "outputId": "b705e678-0e15-4c2d-b122-964b3bb10319"
      },
      "source": [
        "# MASK TOEKN看看\n",
        "\n",
        "mask_token_id = vectorize_layer([['[mask]']]).numpy()[0][0]   # return (1, max_len)\n",
        "mask_token_id"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIQH7-NZ7p3F"
      },
      "source": [
        "def encode(texts):\n",
        "    \"\"\"\n",
        "        將文字輸入做處理、轉換成模型可處理的vector，也可將其變成end-to-end model，\n",
        "        此方法是先在資料集處理。\n",
        "    \"\"\"\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FmAlAesygg8",
        "outputId": "078807c3-443e-46a2-8956-275cc17da440"
      },
      "source": [
        "encode(train_df.review.values)      # 輸入轉換成向量"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,    6,    2, ...,    0,    0,    0],\n",
              "       [   9,  230,   10, ...,    5,  797,  342],\n",
              "       [   2,    1,  579, ...,  876,    5,    2],\n",
              "       ...,\n",
              "       [   9,  116,   10, ...,    0,    0,    0],\n",
              "       [  10,    7,    2, ...,    0,    0,    0],\n",
              "       [   9,  214, 2953, ..., 4400,    8, 1949]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iko0DmbEy_Lc",
        "outputId": "f5480347-465e-436b-bffd-e8883f912e21"
      },
      "source": [
        "vectorize_layer.get_vocabulary()[:3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcs06CA4aCY2",
        "outputId": "d6fda0d7-10e3-44d3-d878-8fd6049bdf24"
      },
      "source": [
        "# masked 總數估計\n",
        "\n",
        "25000 * 256 * 0.15"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "960000.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nrZeJjxeUYD"
      },
      "source": [
        "### masked inputs and labels 釋例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TiDl4oVXTYW"
      },
      "source": [
        "### masked inputs and labels 釋例\n",
        "\n",
        "encoded_texts = encode(train_df.review.values)  # (25000, 256) ---> 25000筆, 句子長度為256, 也就是256個word。\n",
        "# print(encoded_texts.shape)\n",
        "\n",
        "input_mask = np.random.rand(*encoded_texts.shape) < 0.15        # (25000, 256) ---> boolean matrix, 根據論文mask 15%\n",
        "input_mask[encoded_texts <= 2] = False                            # ['', '[UNK]', 'the'] 是不去mask的。\n",
        "\n",
        "labels = (-1) * np.ones(encoded_texts.shape, dtype=int)         # ---> 真實mask的labels, -1代表忽略\n",
        "labels[input_mask] = encoded_texts[input_mask]                  # 給予真實的word token, 真正masked的資料之答案\n",
        "\n",
        "# print(labels[input_mask].shape)                                 # 586446, 可能每次會不一樣, 根據seed, 代表的是: 在訓練語言模型時, 會預測的Word總數。\n",
        "#### 上面完成了mask資料的y部分(預測的地方)\n",
        "\n",
        "#### 輸入部分\n",
        "encoded_texts_masked = np.copy(encoded_texts)       # 複製, 要修改\n",
        "input_mask_2mask = input_mask & (np.random.rand(*encoded_texts.shape) < 0.90)       # (25000, 256) 的boolean matrix, 根據論文15%裡面有90%使用[mask], \n",
        "encoded_texts_masked[input_mask_2mask] = mask_token_id          # 將其word token改成[mask]的token, 也就是29999, 數量會是上面596446的大概9成: 528488\n",
        "\n",
        "# 剩下10%去隨機token\n",
        "input_mask_2random = input_mask * (np.random.rand(*encoded_texts.shape) < 0.10)\n",
        "encoded_texts_masked[input_mask_2random] = np.random.randint(\n",
        "    low=3,\n",
        "    high=mask_token_id,\n",
        "    size=input_mask_2random.sum()\n",
        ")\n",
        "\n",
        "## 最後一步: 樣本權重設定, 一般是數值weight, 0代表忽略\n",
        "sample_weights = np.ones(labels.shape)              # (25000, 256)\n",
        "sample_weights[labels == -1] = 0\n",
        "\n",
        "y_labels = np.copy(encoded_texts)                   # (25000, 256), 原始句子, 也就是真實答案"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wwjd-JQecEZ"
      },
      "source": [
        "#### Masked之後的輸入token\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRKXMrfqeaaq",
        "outputId": "3da629ec-1bc7-4319-c2c5-52a0fc6adc36"
      },
      "source": [
        "encoded_texts_masked, encoded_texts_masked.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[    1,     6,     2, ...,     0,     0,     0],\n",
              "        [29999,   230,    10, ...,     5,   797,   342],\n",
              "        [    2,     1,   579, ...,   876,     5,     2],\n",
              "        ...,\n",
              "        [    9,   116,    10, ...,     0,     0,     0],\n",
              "        [   10,     7,     2, ...,     0,     0,     0],\n",
              "        [    9,   214,  2953, ...,  4400,     8, 29999]]), (25000, 256))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG_pKwSXetuO"
      },
      "source": [
        "#### 真實labels, 原始句子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Rmd1P_ex7n",
        "outputId": "fd88918b-7965-4b88-b62c-1ea90b1f626f"
      },
      "source": [
        "y_labels, y_labels.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[   1,    6,    2, ...,    0,    0,    0],\n",
              "        [   9,  230,   10, ...,    5,  797,  342],\n",
              "        [   2,    1,  579, ...,  876,    5,    2],\n",
              "        ...,\n",
              "        [   9,  116,   10, ...,    0,    0,    0],\n",
              "        [  10,    7,    2, ...,    0,    0,    0],\n",
              "        [   9,  214, 2953, ..., 4400,    8, 1949]]), (25000, 256))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF5stsHAe0Iy"
      },
      "source": [
        "#### 樣本權重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyGVR8cVe4Qg",
        "outputId": "7122569e-9955-414f-af5b-4563d0fde540"
      },
      "source": [
        "sample_weights, sample_weights.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.]]), (25000, 256))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BavYDLFDxNY1"
      },
      "source": [
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    # 準備label(target)\n",
        "    # 根據BERT論文, 將15%的資料做masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15      # boolean matrix\n",
        "    inp_mask[encoded_texts <= 2] = False                        # <=2的token是特殊token('', '[UNK]'),不mask, 但這邊好像把'the'也不mask了\n",
        "    # 將目標預設為-1,  代表忽略\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]                  # 這就是被mask的真實label, 其餘為-1是忽略的\n",
        "\n",
        "    # 準備輸入\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # 根據BERT論文, 將15%將要mask的資料裡面, 90%去真正使用[mask]\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[inp_mask_2mask] = mask_token_id        # 真正mask的資料, x_data\n",
        "\n",
        "    # 剩下10%去random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 0.10)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        low=3, high=mask_token_id, size=inp_mask_2random.sum()\n",
        "    )       # token(3~mask_token_id(不包含)), 都可能\n",
        "\n",
        "    # 準備樣本權重去pass to fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0            # 忽略的權重會直接變成0, 就不會更新該樣本權重\n",
        "\n",
        "    y_labels = np.copy(encoded_texts)           # y_label, 真實答案\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NBadpY40Ic1"
      },
      "source": [
        "# 訓練資料準備\n",
        "\n",
        "# 這是語義分析的資料\n",
        "x_train = encode(train_df.review.values)\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        tensors=(x_train, y_train)\n",
        "    ).shuffle(1000).batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df.review.values, y_test)\n",
        ").batch(config.BATCH_SIZE)\n",
        "\n",
        "\n",
        "# MLM 訓練資料\n",
        "\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ").shuffle(1000).batch(config.BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jPbDQu0_063"
      },
      "source": [
        "### BERT model建立"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPV6B9gl97dy"
      },
      "source": [
        "def bert_module(query, key, value, i):\n",
        "    \"\"\"\n",
        "        Encoder block\n",
        "    \"\"\"\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=f'encoder_{i}/multiheadattention'\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(rate=0.1, name=f'encoder_{i}/att_dropout')(attention_output)\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-1, name=f'encoder_{i}/att_layernormalization'\n",
        "    )(query + attention_output)                 # short-cut\n",
        "\n",
        "    # ffn\n",
        "    ffn = keras.Sequential([\n",
        "        layers.Dense(config.FF_DIM, activation='relu'),\n",
        "        layers.Dense(config.EMBED_DIM),          \n",
        "    ], name=f\"encoder_{i}/ffn\")\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=f\"encoder_{i}/ffn_dropout\")(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=f\"encoder_{i}/ffn_layernormalization\"\n",
        "    )(attention_output + ffn_output)            # short-cut\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    \"\"\"\n",
        "        初始化的參數, 放入postional embedding讓其繼續學習。\n",
        "    \"\"\"\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")       # : Computes the (weighted) mean of the given values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmUKcbQVH6Jz"
      },
      "source": [
        "#### [sample_weight](https://keras.io/zh/models/model/)\n",
        "- 就是在說訓練中, 樣本權重會使用、注意哪一些，常用在序列相關的資料中!mask data也是!\n",
        "#### [Model.fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit?hl=zh-cn#%E6%94%AF%E6%8C%81_sample_weight_%E5%92%8C_class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0jE2Mk_G6uQ"
      },
      "source": [
        "# MLM 模型\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        # override 訓練步驟, mode.fit()的客製化\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "        \n",
        "        # tf梯度紀錄, forward\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions =  self(features, training=True)        # 前面建好的model layers\n",
        "            loss = loss_fn(\n",
        "                y_true=labels,\n",
        "                y_pred=predictions,\n",
        "                sample_weight=sample_weight                     # 重點之一, model.compile也可以設定, 但不知道效果一樣嗎?\n",
        "            )\n",
        "        \n",
        "        # 梯度計算, backward\n",
        "        trainable_vars = self.trainable_variables               # Model的模型訓練參數\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # 梯度更新\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # 計算model metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)    # 重點之一, metrics設定那些才列入計算\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}          # 平常使用keras看到的高階API, return dict(metrics: value)\n",
        "    \n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STVwrv5IZ2lY",
        "outputId": "b64c7fe0-4d4c-408a-cd8f-fa2da09bb7c2"
      },
      "source": [
        "mask_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QULLsVpWKy3k",
        "outputId": "68b73786-ac08-46b4-93d0-7136d9a6da70"
      },
      "source": [
        "def create_masked_language_bert_model():\n",
        "    \"\"\"\n",
        "        建造MLM with BERT\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(config.MAX_LEN,), dtype='int64')\n",
        "    word_embedding = layers.Embedding(\n",
        "        input_dim=config.VOCAB_SIZE,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        name='word_embedding'\n",
        "    )(inputs)\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=config.MAX_LEN,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],    # 完全讓其自己學也可以\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    # 其實還需要一個segment embedding, 但沒有NSP任務, 先忽略\n",
        "    embedding =  word_embedding + position_embeddings\n",
        "    encoder_output = embedding\n",
        "    \n",
        "    # Encoder block的堆疊\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name='mlm_cls', activation='softmax')(encoder_output)\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name='masked_bert_model')\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)      # 不需要loss 是因為前面MLM已經設定了\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "# token <-> index轉換的dict\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "        Callback使用, 每一個epoch之後的操作\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    # 先要override的method, 在epoch結束會做的事情! 觀察用\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)     # (batch_size=1, 256, 30000)\n",
        "        print(prediction.shape)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id) \n",
        "        print(masked_index)\n",
        "        masked_index = masked_index[1]                  # 觀察一句就好!\n",
        "        print(masked_index)\n",
        "        mask_prediction = prediction[0][masked_index]   # mask的預測值, 因為是分類, 有vocab_size這麼多個, 找top_k個就好!\n",
        "        print(mask_prediction)\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        print(top_indices)\n",
        "        values = mask_prediction[0][top_indices]\n",
        "        print(values)\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]                  # index, 也就是id\n",
        "            v = values[i]                       # 機率\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 256, 128)     3840000     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_100 (TFOpL (None, 256, 128)     0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/multiheadattention (M (None, 256, 128)     66048       tf.__operators__.add_100[0][0]   \n",
            "                                                                 tf.__operators__.add_100[0][0]   \n",
            "                                                                 tf.__operators__.add_100[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_dropout (Dropout) (None, 256, 128)     0           encoder_0/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_101 (TFOpL (None, 256, 128)     0           tf.__operators__.add_100[0][0]   \n",
            "                                                                 encoder_0/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_101[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn (Sequential)      (None, 256, 128)     33024       encoder_0/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_0/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_102 (TFOpL (None, 256, 128)     0           encoder_0/att_layernormalization[\n",
            "                                                                 encoder_0/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_102[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1/multiheadattention (M (None, 256, 128)     66048       encoder_0/ffn_layernormalization[\n",
            "                                                                 encoder_0/ffn_layernormalization[\n",
            "                                                                 encoder_0/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_1/att_dropout (Dropout) (None, 256, 128)     0           encoder_1/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_103 (TFOpL (None, 256, 128)     0           encoder_0/ffn_layernormalization[\n",
            "                                                                 encoder_1/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_103[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1/ffn (Sequential)      (None, 256, 128)     33024       encoder_1/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_1/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_1/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_104 (TFOpL (None, 256, 128)     0           encoder_1/att_layernormalization[\n",
            "                                                                 encoder_1/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_104[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2/multiheadattention (M (None, 256, 128)     66048       encoder_1/ffn_layernormalization[\n",
            "                                                                 encoder_1/ffn_layernormalization[\n",
            "                                                                 encoder_1/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_2/att_dropout (Dropout) (None, 256, 128)     0           encoder_2/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_105 (TFOpL (None, 256, 128)     0           encoder_1/ffn_layernormalization[\n",
            "                                                                 encoder_2/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_105[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2/ffn (Sequential)      (None, 256, 128)     33024       encoder_2/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_2/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_2/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_106 (TFOpL (None, 256, 128)     0           encoder_2/att_layernormalization[\n",
            "                                                                 encoder_2/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_106[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3/multiheadattention (M (None, 256, 128)     66048       encoder_2/ffn_layernormalization[\n",
            "                                                                 encoder_2/ffn_layernormalization[\n",
            "                                                                 encoder_2/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_3/att_dropout (Dropout) (None, 256, 128)     0           encoder_3/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_107 (TFOpL (None, 256, 128)     0           encoder_2/ffn_layernormalization[\n",
            "                                                                 encoder_3/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_107[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3/ffn (Sequential)      (None, 256, 128)     33024       encoder_3/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_3/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_3/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_108 (TFOpL (None, 256, 128)     0           encoder_3/att_layernormalization[\n",
            "                                                                 encoder_3/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_108[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_4/multiheadattention (M (None, 256, 128)     66048       encoder_3/ffn_layernormalization[\n",
            "                                                                 encoder_3/ffn_layernormalization[\n",
            "                                                                 encoder_3/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_4/att_dropout (Dropout) (None, 256, 128)     0           encoder_4/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_109 (TFOpL (None, 256, 128)     0           encoder_3/ffn_layernormalization[\n",
            "                                                                 encoder_4/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_4/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_109[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_4/ffn (Sequential)      (None, 256, 128)     33024       encoder_4/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_4/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_4/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_110 (TFOpL (None, 256, 128)     0           encoder_4/att_layernormalization[\n",
            "                                                                 encoder_4/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_4/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_110[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_5/multiheadattention (M (None, 256, 128)     66048       encoder_4/ffn_layernormalization[\n",
            "                                                                 encoder_4/ffn_layernormalization[\n",
            "                                                                 encoder_4/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_5/att_dropout (Dropout) (None, 256, 128)     0           encoder_5/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_111 (TFOpL (None, 256, 128)     0           encoder_4/ffn_layernormalization[\n",
            "                                                                 encoder_5/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_5/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_111[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_5/ffn (Sequential)      (None, 256, 128)     33024       encoder_5/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_5/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_5/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_112 (TFOpL (None, 256, 128)     0           encoder_5/att_layernormalization[\n",
            "                                                                 encoder_5/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_5/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_112[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_6/multiheadattention (M (None, 256, 128)     66048       encoder_5/ffn_layernormalization[\n",
            "                                                                 encoder_5/ffn_layernormalization[\n",
            "                                                                 encoder_5/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_6/att_dropout (Dropout) (None, 256, 128)     0           encoder_6/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_113 (TFOpL (None, 256, 128)     0           encoder_5/ffn_layernormalization[\n",
            "                                                                 encoder_6/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_6/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_113[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_6/ffn (Sequential)      (None, 256, 128)     33024       encoder_6/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_6/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_6/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_114 (TFOpL (None, 256, 128)     0           encoder_6/att_layernormalization[\n",
            "                                                                 encoder_6/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_6/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_114[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_7/multiheadattention (M (None, 256, 128)     66048       encoder_6/ffn_layernormalization[\n",
            "                                                                 encoder_6/ffn_layernormalization[\n",
            "                                                                 encoder_6/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_7/att_dropout (Dropout) (None, 256, 128)     0           encoder_7/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_115 (TFOpL (None, 256, 128)     0           encoder_6/ffn_layernormalization[\n",
            "                                                                 encoder_7/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_7/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_115[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_7/ffn (Sequential)      (None, 256, 128)     33024       encoder_7/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_7/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_7/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_116 (TFOpL (None, 256, 128)     0           encoder_7/att_layernormalization[\n",
            "                                                                 encoder_7/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_7/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_116[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_8/multiheadattention (M (None, 256, 128)     66048       encoder_7/ffn_layernormalization[\n",
            "                                                                 encoder_7/ffn_layernormalization[\n",
            "                                                                 encoder_7/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_8/att_dropout (Dropout) (None, 256, 128)     0           encoder_8/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_117 (TFOpL (None, 256, 128)     0           encoder_7/ffn_layernormalization[\n",
            "                                                                 encoder_8/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_8/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_117[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_8/ffn (Sequential)      (None, 256, 128)     33024       encoder_8/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_8/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_8/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_118 (TFOpL (None, 256, 128)     0           encoder_8/att_layernormalization[\n",
            "                                                                 encoder_8/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_8/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_118[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_9/multiheadattention (M (None, 256, 128)     66048       encoder_8/ffn_layernormalization[\n",
            "                                                                 encoder_8/ffn_layernormalization[\n",
            "                                                                 encoder_8/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_9/att_dropout (Dropout) (None, 256, 128)     0           encoder_9/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_119 (TFOpL (None, 256, 128)     0           encoder_8/ffn_layernormalization[\n",
            "                                                                 encoder_9/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_9/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_119[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_9/ffn (Sequential)      (None, 256, 128)     33024       encoder_9/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_9/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_9/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_120 (TFOpL (None, 256, 128)     0           encoder_9/att_layernormalization[\n",
            "                                                                 encoder_9/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_9/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_120[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_10/multiheadattention ( (None, 256, 128)     66048       encoder_9/ffn_layernormalization[\n",
            "                                                                 encoder_9/ffn_layernormalization[\n",
            "                                                                 encoder_9/ffn_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_10/att_dropout (Dropout (None, 256, 128)     0           encoder_10/multiheadattention[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_121 (TFOpL (None, 256, 128)     0           encoder_9/ffn_layernormalization[\n",
            "                                                                 encoder_10/att_dropout[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_10/att_layernormalizati (None, 256, 128)     256         tf.__operators__.add_121[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_10/ffn (Sequential)     (None, 256, 128)     33024       encoder_10/att_layernormalization\n",
            "__________________________________________________________________________________________________\n",
            "encoder_10/ffn_dropout (Dropout (None, 256, 128)     0           encoder_10/ffn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_122 (TFOpL (None, 256, 128)     0           encoder_10/att_layernormalization\n",
            "                                                                 encoder_10/ffn_dropout[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_10/ffn_layernormalizati (None, 256, 128)     256         tf.__operators__.add_122[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_11/multiheadattention ( (None, 256, 128)     66048       encoder_10/ffn_layernormalization\n",
            "                                                                 encoder_10/ffn_layernormalization\n",
            "                                                                 encoder_10/ffn_layernormalization\n",
            "__________________________________________________________________________________________________\n",
            "encoder_11/att_dropout (Dropout (None, 256, 128)     0           encoder_11/multiheadattention[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_123 (TFOpL (None, 256, 128)     0           encoder_10/ffn_layernormalization\n",
            "                                                                 encoder_11/att_dropout[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_11/att_layernormalizati (None, 256, 128)     256         tf.__operators__.add_123[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_11/ffn (Sequential)     (None, 256, 128)     33024       encoder_11/att_layernormalization\n",
            "__________________________________________________________________________________________________\n",
            "encoder_11/ffn_dropout (Dropout (None, 256, 128)     0           encoder_11/ffn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_124 (TFOpL (None, 256, 128)     0           encoder_11/att_layernormalization\n",
            "                                                                 encoder_11/ffn_dropout[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_11/ffn_layernormalizati (None, 256, 128)     256         tf.__operators__.add_124[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "mlm_cls (Dense)                 (None, 256, 30000)   3870000     encoder_11/ffn_layernormalization\n",
            "==================================================================================================\n",
            "Total params: 8,905,008\n",
            "Trainable params: 8,905,008\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3XuL2G7Z0Y8"
      },
      "source": [
        "for id_ in sample_tokens[0]:\n",
        "    print(id_.numpy(), id2token[id_.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJLt-rXENwmw",
        "outputId": "1004a80d-48a2-4c0f-8cec-2e9a4cc241a7"
      },
      "source": [
        "# MLM訓練\n",
        "\n",
        "\n",
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 529s 332ms/step - loss: 7.2952\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[1.1767518e-07 9.7411871e-08 1.1453801e-07 ... 1.4810685e-06\n",
            "  2.2545164e-06 1.0465070e-07]]\n",
            "[3 4 5 6 7]\n",
            "[0.03327482 0.03260406 0.02907146 0.02707672 0.0211609 ]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.033274822}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'and',\n",
            " 'prediction': 'i have watched this and and it was awesome',\n",
            " 'probability': 0.032604057}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.02907146}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'to',\n",
            " 'prediction': 'i have watched this to and it was awesome',\n",
            " 'probability': 0.027076721}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.0211609}\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 519s 332ms/step - loss: 7.2117\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[5.0928051e-08 4.2893163e-08 5.0339381e-08 ... 1.3905850e-06\n",
            "  3.6742392e-06 4.4945477e-08]]\n",
            "[4 3 5 6 7]\n",
            "[0.03585356 0.0351074  0.02763853 0.02620368 0.01884027]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'and',\n",
            " 'prediction': 'i have watched this and and it was awesome',\n",
            " 'probability': 0.03585356}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.0351074}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.027638532}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'to',\n",
            " 'prediction': 'i have watched this to and it was awesome',\n",
            " 'probability': 0.026203685}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.018840266}\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 518s 332ms/step - loss: 7.1864\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[2.7752989e-08 2.3197005e-08 2.7476940e-08 ... 1.0574914e-06\n",
            "  6.9176499e-06 2.3543189e-08]]\n",
            "[4 3 5 6 9]\n",
            "[0.03634311 0.03434702 0.02862019 0.02721827 0.02389965]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'and',\n",
            " 'prediction': 'i have watched this and and it was awesome',\n",
            " 'probability': 0.03634311}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.03434702}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.028620195}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'to',\n",
            " 'prediction': 'i have watched this to and it was awesome',\n",
            " 'probability': 0.027218267}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.023899648}\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 518s 332ms/step - loss: 7.1676\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[1.5200712e-08 1.2868999e-08 1.5223067e-08 ... 1.0754328e-06\n",
            "  8.4392896e-06 1.2909834e-08]]\n",
            "[3 4 5 6 9]\n",
            "[0.03646959 0.03393833 0.03020335 0.02753354 0.01938725]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.036469594}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'and',\n",
            " 'prediction': 'i have watched this and and it was awesome',\n",
            " 'probability': 0.03393833}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.030203346}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'to',\n",
            " 'prediction': 'i have watched this to and it was awesome',\n",
            " 'probability': 0.027533539}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.019387245}\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 519s 332ms/step - loss: 7.1542\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[7.0795023e-09 5.8254059e-09 6.9723241e-09 ... 7.3852289e-07\n",
            "  1.4188555e-05 5.9673289e-09]]\n",
            "[3 4 6 5 7]\n",
            "[0.04048523 0.03460959 0.03333694 0.03298013 0.0241338 ]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.04048523}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'and',\n",
            " 'prediction': 'i have watched this and and it was awesome',\n",
            " 'probability': 0.034609586}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'to',\n",
            " 'prediction': 'i have watched this to and it was awesome',\n",
            " 'probability': 0.03333694}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.03298013}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.024133796}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMSMM_G9kd9o"
      },
      "source": [
        "### 結果\n",
        "- 根據觀察, cloze task並沒有訓練好, 故合理推測其實應沒有學習到應有的語意關係\n",
        "- 可能原因\n",
        "    - Encoder 太深\n",
        "    - 訓練資料不夠多"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUnuA19wPhqe"
      },
      "source": [
        "### BERT的強大 - Finetune\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJoCEf5ZPVul",
        "outputId": "b9bc0d63-bd50-4b99-d7c8-43e3e19afb33"
      },
      "source": [
        "# 使用預訓練模型\n",
        "mlm_model = keras.models.load_model(\n",
        "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
        ")\n",
        "\n",
        "pretrained_bert_model = tf.keras.Model(\n",
        "    mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output # 到這邊剛好是MLM分類任務前, 也就是學到的語意關係\n",
        ")\n",
        "\n",
        "# 凍結之前訓練的權重, 可選擇\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "    sequence_output = pretrained_bert_model(inputs)\n",
        "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)        # 類似flattn, 原先3維\n",
        "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)   \n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)       # 二分類問題, 故Dense(1)\n",
        "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    classifer_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return classifer_model\n",
        "\n",
        "classifer_model = create_classifier_bert_model()\n",
        "classifer_model.summary()\n",
        "\n",
        "# Train the classifier with frozen BERT stage(凍結下的訓練)\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")\n",
        "\n",
        "# Unfreeze the BERT model for fine-tuning(不凍結下訓練, 代表MLM的參數也會跟著變動)\n",
        "pretrained_bert_model.trainable = True\n",
        "optimizer = keras.optimizers.Adam()\n",
        "classifer_model.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 256)]             0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 256, 128)          3939584   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,947,905\n",
            "Trainable params: 8,321\n",
            "Non-trainable params: 3,939,584\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 15s 18ms/step - loss: 0.7165 - accuracy: 0.5016 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.7002 - accuracy: 0.5031 - val_loss: 0.6987 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6979 - accuracy: 0.5004 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6971 - accuracy: 0.4932 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6957 - accuracy: 0.4937 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 46s 57ms/step - loss: 0.6964 - accuracy: 0.4980 - val_loss: 0.7738 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 44s 57ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.7762 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 45s 57ms/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.7829 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 45s 58ms/step - loss: 0.6932 - accuracy: 0.4966 - val_loss: 0.7551 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 45s 57ms/step - loss: 0.6932 - accuracy: 0.4965 - val_loss: 0.7546 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66983cae50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0-8CejpQa0y"
      },
      "source": [
        "### end-to-end\n",
        "- 之前的textvector之在資料集方面處理的, 也可以透過把layer embed 到model內, 得到end-to-end的模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK3ezbsrQmCM",
        "outputId": "5d4ea8ce-787e-433f-c0f0-8dba43a7b365"
      },
      "source": [
        "def get_end_to_end(model):\n",
        "    \"\"\"\n",
        "        基於原先的模型, 將輸入處多放入一個\"文本處理\"layer也就是: TextVectorization得到的layer,\n",
        "        就可以得到end-to-end model\n",
        "    \"\"\"\n",
        "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")     # 文本(inputs)\n",
        "    indices = vectorize_layer(inputs_string)                    # 轉成向量\n",
        "    outputs = model(indices)                                    # outputs\n",
        "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    end_to_end_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return end_to_end_model\n",
        "\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 9s 10ms/step - loss: 0.7526 - accuracy: 0.5028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7545887231826782, 0.5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6O4_D98mIKp"
      },
      "source": [
        "### 修正\n",
        "- Encoder\n",
        "    12 -> 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLBaHurcmhlO",
        "outputId": "5c5bc28e-b1e6-42f9-c3f7-e7ea92bd4f51"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 128\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()\n",
        "config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVwPtdZhRL5Q",
        "outputId": "3a9e6842-2564-427b-eb17-20e3cc91132f"
      },
      "source": [
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model_1layer = create_masked_language_bert_model()\n",
        "bert_masked_model_1layer.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 256, 128)     3840000     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_125 (TFOpL (None, 256, 128)     0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/multiheadattention (M (None, 256, 128)     66048       tf.__operators__.add_125[0][0]   \n",
            "                                                                 tf.__operators__.add_125[0][0]   \n",
            "                                                                 tf.__operators__.add_125[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_dropout (Dropout) (None, 256, 128)     0           encoder_0/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_126 (TFOpL (None, 256, 128)     0           tf.__operators__.add_125[0][0]   \n",
            "                                                                 encoder_0/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_126[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn (Sequential)      (None, 256, 128)     33024       encoder_0/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_0/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_127 (TFOpL (None, 256, 128)     0           encoder_0/att_layernormalization[\n",
            "                                                                 encoder_0/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_127[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "mlm_cls (Dense)                 (None, 256, 30000)   3870000     encoder_0/ffn_layernormalization[\n",
            "==================================================================================================\n",
            "Total params: 7,809,584\n",
            "Trainable params: 7,809,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAWh3lgjmtol",
        "outputId": "b955a5a3-ea05-4dbb-fc2d-a1bfb9ef6b80"
      },
      "source": [
        "bert_masked_model_1layer.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model_1layer.save(\"bert_mlm_imdb_1layer.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 237s 151ms/step - loss: 7.1679\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f66bcf44b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[6.066066e-08 7.209404e-08 6.055639e-08 ... 2.872794e-06 5.329966e-07\n",
            "  7.219587e-08]]\n",
            "[10  9  3 19  5]\n",
            "[0.04216674 0.03485606 0.03185284 0.02736021 0.01999529]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'this',\n",
            " 'prediction': 'i have watched this this and it was awesome',\n",
            " 'probability': 0.042166743}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.03485606}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.03185284}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.027360214}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.019995295}\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 235s 151ms/step - loss: 6.7003\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[1.5941689e-08 1.7956349e-08 1.6543915e-08 ... 3.2054832e-06\n",
            "  8.1048036e-08 2.0439359e-08]]\n",
            "[  24   19    7 1371  236]\n",
            "[0.1442383  0.12774321 0.03840918 0.03824802 0.02276796]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.1442383}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.12774321}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.038409185}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie!',\n",
            " 'prediction': 'i have watched this movie! and it was awesome',\n",
            " 'probability': 0.03824802}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'worst',\n",
            " 'prediction': 'i have watched this worst and it was awesome',\n",
            " 'probability': 0.022767955}\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 235s 151ms/step - loss: 6.1178\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[2.9248046e-09 3.0976881e-09 2.8288334e-09 ... 4.0698313e-07\n",
            "  8.3580813e-09 3.1841361e-09]]\n",
            "[  19 1371   24  236   28]\n",
            "[0.24436265 0.10856409 0.0889897  0.03967827 0.02818749]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.24436265}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie!',\n",
            " 'prediction': 'i have watched this movie! and it was awesome',\n",
            " 'probability': 0.10856409}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.088989705}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'worst',\n",
            " 'prediction': 'i have watched this worst and it was awesome',\n",
            " 'probability': 0.03967827}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.028187485}\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 236s 151ms/step - loss: 5.6985\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[3.6879197e-10 3.9983994e-10 3.6787795e-10 ... 1.0033107e-07\n",
            "  1.0423412e-09 3.7345885e-10]]\n",
            "[  19   24 1371  103  122]\n",
            "[0.5538519  0.130241   0.07007366 0.04236298 0.01845231]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.5538519}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.130241}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie!',\n",
            " 'prediction': 'i have watched this movie! and it was awesome',\n",
            " 'probability': 0.07007366}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie.',\n",
            " 'prediction': 'i have watched this movie. and it was awesome',\n",
            " 'probability': 0.04236298}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film.',\n",
            " 'prediction': 'i have watched this film. and it was awesome',\n",
            " 'probability': 0.018452313}\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 236s 151ms/step - loss: 5.2289\n",
            "(1, 256, 30000)\n",
            "(array([0]), array([4]))\n",
            "[4]\n",
            "[[1.0933817e-11 1.2226158e-11 1.0941828e-11 ... 1.0728989e-07\n",
            "  2.4392388e-10 1.2207028e-11]]\n",
            "[  19   24 1048  103  789]\n",
            "[0.4460333  0.33436406 0.01941711 0.01724243 0.01374847]\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.4460333}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.33436406}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'sequel',\n",
            " 'prediction': 'i have watched this sequel and it was awesome',\n",
            " 'probability': 0.019417105}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie.',\n",
            " 'prediction': 'i have watched this movie. and it was awesome',\n",
            " 'probability': 0.017242432}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'flick',\n",
            " 'prediction': 'i have watched this flick and it was awesome',\n",
            " 'probability': 0.013748465}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXmCGABenCXv",
        "outputId": "a1af46f1-d04a-47dd-9a7b-274bfd43b31e"
      },
      "source": [
        "def user_bert_for_pretrained(model_file='bert_mlm_imdb_1layer.h5'):# 使用預訓練模型\n",
        "    mlm_model = keras.models.load_model(\n",
        "        model_file, custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
        "    )\n",
        "\n",
        "    pretrained_bert_model = tf.keras.Model(\n",
        "        mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output # 到這邊剛好是MLM分類任務前, 也就是學到的語意關係\n",
        "    )\n",
        "\n",
        "    # 凍結之前訓練的權重, 可選擇\n",
        "    pretrained_bert_model.trainable = False\n",
        "\n",
        "    def create_classifier_bert_model():\n",
        "        inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "        sequence_output = pretrained_bert_model(inputs)\n",
        "        pooled_output = layers.GlobalMaxPooling1D()(sequence_output)        # 類似flattn, 原先3維\n",
        "        hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)   \n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)       # 二分類問題, 故Dense(1)\n",
        "        classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "        optimizer = keras.optimizers.Adam()\n",
        "        classifer_model.compile(\n",
        "            optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "        )\n",
        "        return classifer_model\n",
        "\n",
        "    classifer_model = create_classifier_bert_model()\n",
        "    classifer_model.summary()\n",
        "\n",
        "    # Train the classifier with frozen BERT stage(凍結下的訓練)\n",
        "    classifer_model.fit(\n",
        "        train_classifier_ds,\n",
        "        epochs=5,\n",
        "        validation_data=test_classifier_ds,\n",
        "    )\n",
        "\n",
        "    # Unfreeze the BERT model for fine-tuning(不凍結下訓練, 代表MLM的參數也會跟著變動)\n",
        "    pretrained_bert_model.trainable = True\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    classifer_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    classifer_model.fit(\n",
        "        train_classifier_ds,\n",
        "        epochs=5,\n",
        "        validation_data=test_classifier_ds,\n",
        "    )\n",
        "    return classifer_model\n",
        "\n",
        "classifier_bert_1layer = user_bert_for_pretrained()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 256)]             0         \n",
            "_________________________________________________________________\n",
            "model_2 (Functional)         (None, 256, 128)          3939584   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,947,905\n",
            "Trainable params: 8,321\n",
            "Non-trainable params: 3,939,584\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 15s 18ms/step - loss: 0.7533 - accuracy: 0.5322 - val_loss: 0.6586 - val_accuracy: 0.6062\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6702 - accuracy: 0.5928 - val_loss: 0.6525 - val_accuracy: 0.6181\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6597 - accuracy: 0.6091 - val_loss: 0.6491 - val_accuracy: 0.6174\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6551 - accuracy: 0.6148 - val_loss: 0.6963 - val_accuracy: 0.5622\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6492 - accuracy: 0.6221 - val_loss: 0.6774 - val_accuracy: 0.5808\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 49s 61ms/step - loss: 0.5724 - accuracy: 0.6957 - val_loss: 0.3667 - val_accuracy: 0.8364\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.3046 - accuracy: 0.8714 - val_loss: 0.3885 - val_accuracy: 0.8316\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.1467 - accuracy: 0.9434 - val_loss: 0.4810 - val_accuracy: 0.8376\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.0619 - accuracy: 0.9792 - val_loss: 0.6785 - val_accuracy: 0.8339\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 47s 59ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.8669 - val_accuracy: 0.8154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyyQGZGqm0Vv",
        "outputId": "f961b44a-41a2-4bad-83df-f737de1df174"
      },
      "source": [
        "classifier_bert_1layer.evaluate(test_classifier_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 6s 8ms/step - loss: 0.8669 - accuracy: 0.8154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8669344782829285, 0.8154000043869019]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMLSpfNGo0sN"
      },
      "source": [
        "### Encoder layers改變結果\n",
        "- 可以發現第一個epoch loss就降得很快, 測試語句也很快的找出正確答案, 可以得知有部分抓取語意。\n",
        "- 另外也發現, 有沒有凍結參數效果差很多, 如果沒有凍結, 效果fine tune得很快, 但結論有待商榷。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CIbYC1ZoJrw"
      },
      "source": [
        "> 下次加超參數fine-tune、NSP任務、wordpiece等等"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0dRfhnPoTGz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
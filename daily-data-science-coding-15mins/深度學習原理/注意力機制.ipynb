{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力機制\n",
    "- 人天然在視覺觀察時就會有注意力的偏重，比如觀察Google網頁時，總是會看左上角先出現的部分，將這樣的觀念帶入到加權概念上，就是注意力機制了。\n",
    "- 不論在NLP、CV都有很多的應用，如Transformers的QKV自注意力機制。\n",
    "- 值得注意的是，注意力機制本質就是加權平均，不過是`動態的`加權平均，比如過往計算平均數是，是透過所有樣本給予相同的權重，或者其他的加權方式都是計算方式已經被固定，也就是`靜態的`；而動態的加權平均是會隨著輸入樣本產生變化，舉實際案例來說，當看到一張狗的圖片時，我們就會聚焦於狗出現的位置，而非其他地方，但狗出現在圖片的位置不一定是在圖片的固定方位，因此是動態的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:35.755573Z",
     "start_time": "2023-04-14T15:19:35.743576Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:36.255571Z",
     "start_time": "2023-04-14T15:19:36.232573Z"
    }
   },
   "outputs": [],
   "source": [
    "# class AttentionLayer(Layer):\n",
    "#     \"\"\"\n",
    "#         這是錯的~因為e是(None, 1)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "#         # 定义可学习的权重参数\n",
    "#         self.w = self.add_weight(shape=(input_shape[-1], 1),\n",
    "#                                  initializer='random_normal',\n",
    "#                                  trainable=True)\n",
    "#         super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         # 计算加权和\n",
    "#         e = tf.keras.backend.dot(inputs, self.w)  # (None, 1)\n",
    "#         a = tf.keras.backend.softmax(e, axis=1)   # (None, 1)\n",
    "#         output = inputs * a                       # (None, 96)  \n",
    "#         print(e.shape, a.shape, output.shape)\n",
    "#         return output\n",
    "\n",
    "    \n",
    "class AttentionLayer2(Layer):\n",
    "    def __init__(self, units=None, **kwargs):\n",
    "        super(AttentionLayer2, self).__init__(**kwargs)\n",
    "        self.dense = keras.layers.Dense(units=units, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.dense(x)  # 出來的就是權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:37.877571Z",
     "start_time": "2023-04-14T15:19:36.826575Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "# 輸入\n",
    "input1 = Input(shape=(10,))\n",
    "input2 = Input(shape=(20,))\n",
    "\n",
    "# functional 模型\n",
    "x1 = Dense(32, activation='relu')(input1)\n",
    "x2 = Dense(64, activation='relu')(input2)\n",
    "x = Concatenate()([x1, x2])\n",
    "# x = AttentionLayer()(x)\n",
    "alpha = AttentionLayer2(units=x1.shape[-1] + x2.shape[-1], name='attention')(x)   # 權重\n",
    "x = keras.layers.Dot(axes=1)([x, alpha])\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 編譯\n",
    "model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:38.033572Z",
     "start_time": "2023-04-14T15:19:37.884574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           352         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           1344        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 96)           0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " attention (AttentionLayer2)    (None, 96)           9312        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['concatenate[0][0]',            \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            2           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,010\n",
      "Trainable params: 11,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:38.564578Z",
     "start_time": "2023-04-14T15:19:38.539572Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "inputs = model.inputs\n",
    "alpha_output = model.get_layer(name='attention').output\n",
    "\n",
    "\n",
    "new_model = keras.Model(inputs=inputs, outputs=alpha_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:39.329574Z",
     "start_time": "2023-04-14T15:19:39.274573Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           352         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           1344        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 96)           0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " attention (AttentionLayer2)    (None, 96)           9312        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,008\n",
      "Trainable params: 11,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:40.480573Z",
     "start_time": "2023-04-14T15:19:40.326571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 96)\n",
      "tf.Tensor(\n",
      "[[0.00800204 0.00719401 0.0110973  ... 0.01759386 0.00772979 0.00537172]\n",
      " [0.00269672 0.0310102  0.00881487 ... 0.01436653 0.00736911 0.0120503 ]\n",
      " [0.00683023 0.00996894 0.00976479 ... 0.03163126 0.00387621 0.01836039]\n",
      " ...\n",
      " [0.006447   0.00826287 0.0176962  ... 0.01358038 0.00488194 0.00829465]\n",
      " [0.0054411  0.00646757 0.01681057 ... 0.01833588 0.00559796 0.02084725]\n",
      " [0.00560729 0.007547   0.00612349 ... 0.01624637 0.00376094 0.00649687]], shape=(10000, 96), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_inputs = np.random.normal(size=(10000, 10))\n",
    "x_input2 = np.random.normal(size=(10000, 20))\n",
    "\n",
    "o = new_model([x_inputs, x_input2])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T15:19:41.556572Z",
     "start_time": "2023-04-14T15:19:41.533571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## 如果注意力機制無誤，任一的樣本的權重要=1\n",
    "\n",
    "import random\n",
    "\n",
    "row = random.choice(o)\n",
    "\n",
    "print(tf.reduce_sum(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
